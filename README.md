# CASCON-2017
Code samples for CASCON 2017 paper & demo:<br/>
**Using IBM Watson cloud services to analyze chat conversations, forum posts, and social media**



<p>&nbsp;</p>
<p>&nbsp;</p>

# Prerequisites

**Note:** You don't have to meet any of these prerequisites to have fun... The directory `sample-output` contains all the output you would have generated if you ran the code yourself, including the html dashboard and clustering results in .pdf files.  :)

But if you want to get into the code, read on:

- IBM Cloud (formerly IBM Bluemix)

  1. If you want to run any sample code, sign up for [IBM Cloud](https://www.ibm.com/cloud).
  
  2. If you want to perform classification (that's files 01 - 04, and 10), provision an instance of the [Watson Natural Language Classifier](https://console.bluemix.net/catalog/services/natural-language-classifier) service (there is a free plan.)
  
  3. If you want to perform general or custom natural language understanding (that's files 05 - 10), provision an instance of the [Watson Natural Language Understanding](https://console.bluemix.net/catalog/services/natural-language-understanding) service.
  
  4. If you want to create a custom language model (that's files 07 - 09), sign up for [Watson Knowledge Studio](https://www.ibm.com/marketplace/supervised-machine-learning) (there's a 30-day free trial.)
  
- On your computer

  1. If you want to run any Node.js sample code, perform these steps:
  
      1. [install Node.js](https://nodejs.org/en/download).
    
      2. After downloading all the files from this repo, install the needed Node.js libraries by issuing the following command from the `sample-code` directory:<br/><p>`npm install`</p><p>(This command will use the file `package.json` to know which libraries are needed.)</p>
    
  2. If you want to perform clustering locally (that's an R script generated by file 13), [install RStudio](https://www.rstudio.com/products/rstudio/download).<br/><p>If you don't want to install RStudio locally, some alternative ways to run the clustering script will be given below.</p>



<p>&nbsp;</p>
<p>&nbsp;</p>

# High-level instructions

Work through the files in the `sample-code` directory in numerical order.  The file names indicate what they do.  The files start small and simple, and get more complex as you go.

### Credentials

- All of the NLC files (01 - 04, and 10) require you to paste a URL, username, and password for your NLC service at the top of the file.  You can retrieve those details from the *service details page* for your NLC service in the IBM Cloud dashboard.

- All of the NLU files (05 - 10) require you to paste a username and password for your NLU service a the top of the file.  You can retrieve those credentials from the *service details page* for your NLU service in the IBM Cloud dashboard.

### Model IDs

- The NLC files that perform classification (03, 04, and 10) require you to paste the classifier ID at the top of the file.  File 01 is what creates the classifier using training data from the `sample-data` directory.  You can retrieve the classifier ID from the output of file 01 or file 02, or from the Watson NLC toolkit that you can launch from the *service details page* for your NLC service in the IBM Cloud dashboard.

- The NLU files that use a custom language model (07 - 10) require you to create a custom language model, and then paste the model ID at the top of the file.  Steps for creating the custom language model are below.  You can retrieve the model ID from Watson Knowledge Studio after you deploy the custom language model, or from the output of file 07.

### Videos

The short videos in the `demo-videos` directory show me working my way through these files, and creating the custom language model, just like you would.

*[2017-11-06] Note: I'm uploading these videos as fast as I can over the next two days.*



<p>&nbsp;</p>
<p>&nbsp;</p>

# Creating the custom language model

**Note:** As described in the disclaimer below, this is not the best practice for creating a robust, enterprise-solution custom language model.  Instead, this is the fastest way possible to go from "no custom language model" to "having a custom language model".

1. Log in to the Watson Knowledge Studio (when you sign up, they send you an email with info about your server and how to log in.)

2. On the **Type System** page, create the type system by adding the following Entity types:

    - action
    - docs
    - obj
    - persona
    - tech

    These don't need Roles or Subtypes.. they just need to exist.

3. On the **Documents** page:

    1. Click *Import Document Set* and then upload all the files in the `custom-language-model/document-set` directory.

   2. Click *Create Annotation Set* and then create an annotation set containing all the files you just uploaded.

4. On the **Dictionaries** page, import all the files in the `custom-language-model/dictionaries` directory.

5.  On the **Annotator Component** page, create a "Dictionary pre-annotator":

    1. When prompted to create a "Dictionary Mapping", associate each entity type name with the dictionary of the same name, and then click *Create & Run*.

    2. When prompted to specify what to pre-annotate, check the box beside the annotation set you just created, and then click *Run*.

        It takes a few minutes to complete.

6.  After the pre-annotation is finished, on the **Human Annotation** page, perform human annotation:

    1. Click *Add Task*.

    2. When prompted, add your annotation set to this task, then click *Create Task*.

    3. Double-click on the new "task" to work with it, and then under the "Action" menu item, click *Annotate*.  This will open a new browser tab.

    4. In the new tab, verify that the dictionary pre-annotator worked properly:<br/><p>The files from the annotation set (those sample data files from the `custom-language-model/document-set` directory) will be displayed.  Click one of them to view the job the dictionary pre-annotator did.  If the dictionary pre-annotator worked properly, you should see some words highlighted in different colours.</p>

    5. If it looks like the dictionary pre-annotator worked properly, close the text file, click *Submit All*.  Then close that new tab.<br/><p>(That's right, you didn't actually do any human annotating.. I did say this is the fast way to get a custom language model built, not the best way - for a given value of "best".)</p>

    6. Back on the **Human Annotation** page, click *Refresh*, check the box beside your annotation set name, and then click *Accept*.

7. On the **Annotator Component** page, create a Machine Learning annotator:

    1. Click *Create Annotator* and then select the Machine Learning type of annotator.

    2. When prompted, check the box beside your annotation set again, and then click *Next*

    3. On the next panel, accept the default choice to reuse the mapping used for the dictionary pre-annotator, and click *Train & Evaluate*.

        It will take a few minutes for the annotator to be ready.

8. When the Machine Learning annotator is done training, deploy the model:

    1. Click "Details" on the Machine Learning annotator card.

    2. On the Machine Learning Model details page, click *Take Snapshot*.

    3. Click *Deploy*.

    4.  Select "Natural Language Understanding" as the type of service to deploy to, then click *Next*.

    5.  Follow the prompts to select your Region, your Space, and your NLU Service Instance from the drop-down lists, then click *Deploy*.

    6. Although it will take a few minutes to deploy, you can see the model ID displayed in a modal window.

Now you're ready to work with file 07_NLU-list-custom-models.js!




<p>&nbsp;</p>
<p>&nbsp;</p>

# Alternatives to installing RStudio locally
Installing RStudio locally is easy, and there's a free version, so that really is a good way to go.  But if you don't want to do that, there are at least two other ways to explore the R script generated by file 13:

- Sign up for a free trial of [Data Science Experience](https://datascience.ibm.com) and then paste the R from the script generated by file 13 into a Notebook.

- Provision an instance of [Db2 Warehouse on Cloud](https://console.bluemix.net/catalog/services/db2-warehouse-on-cloud) and then use the REST API to run the R script generated by file 13 on the Db2 Warehouse on Cloud server.


<p>&nbsp;</p>
<p>&nbsp;</p>

# Disclaimer

What's shown in these files is not the "best practice" or the recommended way to do things.  Instead, what's in these files is just a *fast* way to step through using these services and to see the value you can get (eg. the dashboard and the clusters.)  For example, there are MUCH better ways to normalize results for natural language understanding projects.. but the kludge here (file 12) requires no extra tools or effort.  :)

This is meant to be fun and to perhaps inspire you to create solutions for processing your comments, questions, and chat convos.

